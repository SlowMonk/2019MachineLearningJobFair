{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.kaggle.com/hmendonca/u-net-model-with-submission\n",
    "\n",
    "# Attribution\n",
    "\n",
    "\n",
    "1. slight deviation from previous Unet model.   \n",
    "2. IOI loss function 적용  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import six\n",
    "\n",
    "from random import randint\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "plt.style.use('seaborn-white')\n",
    "import seaborn as sns\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skimage.transform import resize\n",
    "\n",
    "\n",
    "from keras import Model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from keras.models import load_model\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils.vis_utils import plot_model\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Input, Conv2D, Conv2DTranspose, MaxPooling2D, concatenate, Dropout,BatchNormalization\n",
    "from keras.layers import Conv2D, Concatenate, MaxPooling2D\n",
    "from keras.layers import UpSampling2D, Dropout, BatchNormalization\n",
    "from tqdm import tqdm_notebook\n",
    "from keras import initializers\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from keras.utils import conv_utils\n",
    "from keras.utils.data_utils import get_file\n",
    "from keras.engine.topology import get_source_inputs\n",
    "from keras.engine import InputSpec\n",
    "from keras import backend as K\n",
    "from keras.regularizers import l2\n",
    "\n",
    "from keras.engine.topology import Input\n",
    "from keras.engine.training import Model\n",
    "from keras.layers.convolutional import Conv2D, UpSampling2D, Conv2DTranspose\n",
    "from keras.layers.core import Activation, SpatialDropout2D\n",
    "from keras.layers.merge import concatenate,add\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.pooling import MaxPooling2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.1\n",
      "1.15.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "import tensorflow as tf\n",
    "print(keras.__version__)\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.test.is_gpu_available(\n",
    "    cuda_only=False, min_cuda_compute_capability=None\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 1906732041320515871\n",
      ", name: \"/device:XLA_CPU:0\"\n",
      "device_type: \"XLA_CPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 12719283003476274046\n",
      "physical_device_desc: \"device: XLA_CPU device\"\n",
      ", name: \"/device:XLA_GPU:0\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 6689571268458865790\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:XLA_GPU:1\"\n",
      "device_type: \"XLA_GPU\"\n",
      "memory_limit: 17179869184\n",
      "locality {\n",
      "}\n",
      "incarnation: 536306107360901005\n",
      "physical_device_desc: \"device: XLA_GPU device\"\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 197132288\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "    link {\n",
      "      device_id: 1\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 14709811841415002358\n",
      "physical_device_desc: \"device: 0, name: TITAN Xp, pci bus id: 0000:0b:00.0, compute capability: 6.1\"\n",
      ", name: \"/device:GPU:1\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11155914752\n",
      "locality {\n",
      "  bus_id: 2\n",
      "  numa_node: 1\n",
      "  links {\n",
      "    link {\n",
      "      type: \"StreamExecutor\"\n",
      "      strength: 1\n",
      "    }\n",
      "  }\n",
      "}\n",
      "incarnation: 5168622558019988262\n",
      "physical_device_desc: \"device: 1, name: TITAN Xp, pci bus id: 0000:42:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "231723 masks found\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00003e153.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001124c7.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000155de5.jpg</td>\n",
       "      <td>264661 17 265429 33 266197 33 266965 33 267733...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000194a2d.jpg</td>\n",
       "      <td>360486 1 361252 4 362019 5 362785 8 363552 10 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000194a2d.jpg</td>\n",
       "      <td>51834 9 52602 9 53370 9 54138 9 54906 9 55674 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ImageId                                      EncodedPixels\n",
       "0  00003e153.jpg                                                NaN\n",
       "1  0001124c7.jpg                                                NaN\n",
       "2  000155de5.jpg  264661 17 265429 33 266197 33 266965 33 267733...\n",
       "3  000194a2d.jpg  360486 1 361252 4 362019 5 362785 8 363552 10 ...\n",
       "4  000194a2d.jpg  51834 9 52602 9 53370 9 54138 9 54906 9 55674 ..."
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage.segmentation import mark_boundaries\n",
    "#from skimage.util.montage import montage2d as montage\n",
    "path = \"/mnt/3CE35B99003D727B/input/kaggle/Airbus Ship Detection Challenge/\"\n",
    "from keras import models, layers\n",
    "\n",
    "masks = pd.read_csv(os.path.join(path + 'train_ship_segmentations_v2.csv'))\n",
    "\n",
    "print(masks.shape[0],'masks found')\n",
    "\n",
    "masks.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\" \n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overview\n",
    "We try here to improve another public U-Net model: https://www.kaggle.com/kmader/baseline-u-net-model-part-1 which shows how to extract the segmentation map for the ships, augment the images and train a simple DNN model to detect them. A few additional tweaks like balancing the ship-count out a little better have also been done.\n",
    "\n",
    "We are using a different loss function (closer to the competition scoring) and also fix and improve some visualisation functions and the submission itself.\n",
    "\n",
    "Model Parameters\n",
    "We might want to adjust these later (or do some hyperparameter optimizations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 48\n",
    "EDGE_CROP = 16\n",
    "GAUSSIAN_NOISE = 0.1\n",
    "UPSAMPLE_MODE = 'SIMPLE'\n",
    "# downsampling inside the network\n",
    "NET_SCALING = (1, 1)\n",
    "# downsampling in preprocessing\n",
    "IMG_SCALING = (3, 3)\n",
    "# number of validation images to use\n",
    "VALID_IMG_COUNT = 900\n",
    "# maximum number of steps_per_epoch in training\n",
    "MAX_TRAIN_STEPS = 9\n",
    "MAX_TRAIN_EPOCHS = 99\n",
    "AUGMENT_BRIGHTNESS = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "from skimage.io import imread\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.cm import get_cmap\n",
    "from skimage.segmentation import mark_boundaries\n",
    "from skimage.util import montage2d as montage\n",
    "from skimage.morphology import binary_opening, disk, label\n",
    "import gc; gc.enable() # memory is tight\n",
    "\n",
    "montage_rgb = lambda x: np.stack([montage(x[:, :, :, i]) for i in range(x.shape[3])], -1)\n",
    "ship_dir = path\n",
    "train_image_dir = os.path.join(ship_dir, 'train_v2')\n",
    "test_image_dir = os.path.join(ship_dir, 'test_v2')\n",
    "\n",
    "def multi_rle_encode(img, **kwargs):\n",
    "    '''\n",
    "    Encode connected regions as separated masks\n",
    "    '''\n",
    "    labels = label(img)\n",
    "    if img.ndim > 2:\n",
    "        return [rle_encode(np.sum(labels==k, axis=2), **kwargs) for k in np.unique(labels[labels>0])]\n",
    "    else:\n",
    "        return [rle_encode(labels==k, **kwargs) for k in np.unique(labels[labels>0])]\n",
    "\n",
    "# ref: https://www.kaggle.com/paulorzp/run-length-encode-and-decode\n",
    "def rle_encode(img, min_max_threshold=1e-3, max_mean_threshold=None):\n",
    "    '''\n",
    "    img: numpy array, 1 - mask, 0 - background\n",
    "    Returns run length as string formated\n",
    "    '''\n",
    "    if np.max(img) < min_max_threshold:\n",
    "        return '' ## no need to encode if it's all zeros\n",
    "    if max_mean_threshold and np.mean(img) > max_mean_threshold:\n",
    "        return '' ## ignore overfilled mask\n",
    "    pixels = img.T.flatten()\n",
    "    pixels = np.concatenate([[0], pixels, [0]])\n",
    "    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n",
    "    runs[1::2] -= runs[::2]\n",
    "    return ' '.join(str(x) for x in runs)\n",
    "\n",
    "def rle_decode(mask_rle, shape=(768, 768)):\n",
    "    '''\n",
    "    mask_rle: run-length as string formated (start length)\n",
    "    shape: (height,width) of array to return \n",
    "    Returns numpy array, 1 - mask, 0 - background\n",
    "    '''\n",
    "    s = mask_rle.split()\n",
    "    starts, lengths = [np.asarray(x, dtype=int) for x in (s[0:][::2], s[1:][::2])]\n",
    "    starts -= 1\n",
    "    ends = starts + lengths\n",
    "    img = np.zeros(shape[0]*shape[1], dtype=np.uint8)\n",
    "    for lo, hi in zip(starts, ends):\n",
    "        img[lo:hi] = 1\n",
    "    return img.reshape(shape).T  # Needed to align to RLE direction\n",
    "\n",
    "def masks_as_image(in_mask_list):\n",
    "    # Take the individual ship masks and create a single mask array for all ships\n",
    "    all_masks = np.zeros((768, 768), dtype = np.uint8)\n",
    "    for mask in in_mask_list:\n",
    "        if isinstance(mask, str):\n",
    "            all_masks |= rle_decode(mask)\n",
    "    return all_masks\n",
    "\n",
    "def masks_as_color(in_mask_list):\n",
    "    # Take the individual ship masks and create a color mask array for each ships\n",
    "    all_masks = np.zeros((768, 768), dtype = np.float)\n",
    "    scale = lambda x: (len(in_mask_list)+x+1) / (len(in_mask_list)*2) ## scale the heatmap image to shift \n",
    "    for i,mask in enumerate(in_mask_list):\n",
    "        if isinstance(mask, str):\n",
    "            all_masks[:,:] += scale(i) * rle_decode(mask)\n",
    "    return all_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81723 masks in 42556 images\n",
      "150000 empty images in 192556 total images\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>EncodedPixels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00003e153.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0001124c7.jpg</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>000155de5.jpg</td>\n",
       "      <td>264661 17 265429 33 266197 33 266965 33 267733...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>000194a2d.jpg</td>\n",
       "      <td>360486 1 361252 4 362019 5 362785 8 363552 10 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>000194a2d.jpg</td>\n",
       "      <td>51834 9 52602 9 53370 9 54138 9 54906 9 55674 ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         ImageId                                      EncodedPixels\n",
       "0  00003e153.jpg                                                NaN\n",
       "1  0001124c7.jpg                                                NaN\n",
       "2  000155de5.jpg  264661 17 265429 33 266197 33 266965 33 267733...\n",
       "3  000194a2d.jpg  360486 1 361252 4 362019 5 362785 8 363552 10 ...\n",
       "4  000194a2d.jpg  51834 9 52602 9 53370 9 54138 9 54906 9 55674 ..."
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks = pd.read_csv(os.path.join(path , 'train_ship_segmentations_v2.csv'))\n",
    "not_empty = pd.notna(masks.EncodedPixels)\n",
    "print(not_empty.sum(), 'masks in', masks[not_empty].ImageId.nunique(), 'images')\n",
    "print((~not_empty).sum(), 'empty images in', masks.ImageId.nunique(), 'total images')\n",
    "masks.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make sure encode/decode works\n",
    "Given the process\n",
    "$$\n",
    "RLE0⟶DecodeImage0⟶EncodeRLE1⟶DecodeImage1\n",
    " $$\n",
    "We want to check if/that  Image0=?Image1  We could check the RLEs as well but that is more tedious. Also depending on how the objects have been labeled we might have different counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check Decoding->Encoding RLE_0: 9 -> RLE_1: 4\n",
      "0 error\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, (ax1, ax2, ax3, ax4) = plt.subplots(1, 4, figsize = (16, 5))\n",
    "rle_0 = masks.query('ImageId==\"00021ddc3.jpg\"')['EncodedPixels']\n",
    "img_0 = masks_as_image(rle_0)\n",
    "ax1.imshow(img_0)\n",
    "ax1.set_title('Mask as image')\n",
    "rle_1 = multi_rle_encode(img_0)\n",
    "img_1 = masks_as_image(rle_1)\n",
    "ax2.imshow(img_1)\n",
    "ax2.set_title('Re-encoded')\n",
    "img_c = masks_as_color(rle_0)\n",
    "ax3.imshow(img_c)\n",
    "ax3.set_title('Masks in colors')\n",
    "img_c = masks_as_color(rle_1)\n",
    "ax4.imshow(img_c)\n",
    "ax4.set_title('Re-encoded in colors')\n",
    "print('Check Decoding->Encoding',\n",
    "      'RLE_0:', len(rle_0), '->',\n",
    "      'RLE_1:', len(rle_1))\n",
    "print(np.sum(img_0 - img_1), 'error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split into training and validation groups\n",
    "We stratify by the number of boats appearing so we have nice balances in each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ImageId</th>\n",
       "      <th>ships</th>\n",
       "      <th>has_ship</th>\n",
       "      <th>has_ship_vec</th>\n",
       "      <th>file_size_kb</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>168490</th>\n",
       "      <td>e01cb7bbb.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>94.662109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>187796</th>\n",
       "      <td>f9c5ec37c.jpg</td>\n",
       "      <td>3</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>137.322266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29913</th>\n",
       "      <td>27b63eb27.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>116.275391</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>107154</th>\n",
       "      <td>8e79805fc.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>218.610352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190471</th>\n",
       "      <td>fd3a3111f.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>113.750977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45737</th>\n",
       "      <td>3d0b41968.jpg</td>\n",
       "      <td>1</td>\n",
       "      <td>1.0</td>\n",
       "      <td>[1.0]</td>\n",
       "      <td>102.771484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>772</th>\n",
       "      <td>0105f1088.jpg</td>\n",
       "      <td>0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>[0.0]</td>\n",
       "      <td>157.336914</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ImageId  ships  has_ship has_ship_vec  file_size_kb\n",
       "168490  e01cb7bbb.jpg      0       0.0        [0.0]     94.662109\n",
       "187796  f9c5ec37c.jpg      3       1.0        [1.0]    137.322266\n",
       "29913   27b63eb27.jpg      0       0.0        [0.0]    116.275391\n",
       "107154  8e79805fc.jpg      1       1.0        [1.0]    218.610352\n",
       "190471  fd3a3111f.jpg      0       0.0        [0.0]    113.750977\n",
       "45737   3d0b41968.jpg      1       1.0        [1.0]    102.771484\n",
       "772     0105f1088.jpg      0       0.0        [0.0]    157.336914"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "masks['ships'] = masks['EncodedPixels'].map(lambda c_row: 1 if isinstance(c_row, str) else 0)\n",
    "unique_img_ids = masks.groupby('ImageId').agg({'ships': 'sum'}).reset_index()\n",
    "unique_img_ids['has_ship'] = unique_img_ids['ships'].map(lambda x: 1.0 if x>0 else 0.0)\n",
    "unique_img_ids['has_ship_vec'] = unique_img_ids['has_ship'].map(lambda x: [x])\n",
    "# some files are too small/corrupt\n",
    "unique_img_ids['file_size_kb'] = unique_img_ids['ImageId'].map(lambda c_img_id: \n",
    "                                                               os.stat(os.path.join(train_image_dir, \n",
    "                                                                                    c_img_id)).st_size/1024)\n",
    "unique_img_ids = unique_img_ids[unique_img_ids['file_size_kb'] > 50] # keep only +50kb files\n",
    "unique_img_ids['file_size_kb'].hist()\n",
    "masks.drop(['ships'], axis=1, inplace=True)\n",
    "unique_img_ids.sample(7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examine Number of Ship Images\n",
    "Here we examine how often ships appear and replace the ones without any ships with 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f4a2c252a90>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "unique_img_ids['ships'].hist(bins=unique_img_ids['ships'].max())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Undersample Empty Images\n",
    "Here we undersample the empty images to get a better balanced group with more ships to try and segment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12788 masks\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "SAMPLES_PER_GROUP = 2000\n",
    "balanced_train_df = unique_img_ids.groupby('ships').apply(lambda x: x.sample(SAMPLES_PER_GROUP) if len(x) > SAMPLES_PER_GROUP else x)\n",
    "balanced_train_df['ships'].hist(bins=balanced_train_df['ships'].max()+1)\n",
    "print(balanced_train_df.shape[0], 'masks')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35364 training masks\n",
      "8838 validation masks\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_ids, valid_ids = train_test_split(balanced_train_df, \n",
    "                 test_size = 0.2, \n",
    "                 stratify = balanced_train_df['ships'])\n",
    "train_df = pd.merge(masks, train_ids)\n",
    "valid_df = pd.merge(masks, valid_ids)\n",
    "print(train_df.shape[0], 'training masks')\n",
    "print(valid_df.shape[0], 'validation masks')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decode all the RLEs into Images\n",
    "We make a generator to produce batches of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_image_gen(in_df, batch_size = BATCH_SIZE):\n",
    "    all_batches = list(in_df.groupby('ImageId'))\n",
    "    out_rgb = []\n",
    "    out_mask = []\n",
    "    while True:\n",
    "        np.random.shuffle(all_batches)\n",
    "        for c_img_id, c_masks in all_batches:\n",
    "            rgb_path = os.path.join(train_image_dir, c_img_id)\n",
    "            c_img = imread(rgb_path)\n",
    "            #print('c_masks->',c_masks)\n",
    "            c_mask = np.expand_dims(masks_as_image(c_masks['EncodedPixels'].values), -1)\n",
    "            if IMG_SCALING is not None:\n",
    "                c_img = c_img[::IMG_SCALING[0], ::IMG_SCALING[1]]\n",
    "                c_mask = c_mask[::IMG_SCALING[0], ::IMG_SCALING[1]]\n",
    "            out_rgb += [c_img]\n",
    "            out_mask += [c_mask]\n",
    "            if len(out_rgb)>=batch_size:\n",
    "                yield np.stack(out_rgb, 0)/255.0, np.stack(out_mask, 0)\n",
    "                out_rgb, out_mask=[], []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x (48, 256, 256, 3) 0.0 1.0\n",
      "y (48, 256, 256, 1) 0 1\n"
     ]
    }
   ],
   "source": [
    "train_gen = make_image_gen(train_df)\n",
    "train_x, train_y = next(train_gen)\n",
    "print('x', train_x.shape, train_x.min(), train_x.max())\n",
    "print('y', train_y.shape, train_y.min(), train_y.max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize = (30, 10))\n",
    "batch_rgb = montage_rgb(train_x)\n",
    "batch_seg = montage(train_y[:, :, :, 0])\n",
    "ax1.imshow(batch_rgb)\n",
    "ax1.set_title('Images')\n",
    "ax2.imshow(batch_seg)\n",
    "ax2.set_title('Segmentations')\n",
    "ax3.imshow(mark_boundaries(batch_rgb, \n",
    "                           batch_seg.astype(int)))\n",
    "ax3.set_title('Outlined Ships')\n",
    "fig.savefig('overview.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make the Validation Set¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "valid_x, valid_y = next(make_image_gen(valid_df, VALID_IMG_COUNT))\n",
    "print(valid_x.shape, valid_y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Augment Data¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "dg_args = dict(featurewise_center = False, \n",
    "                  samplewise_center = False,\n",
    "                  rotation_range = 45, \n",
    "                  width_shift_range = 0.1, \n",
    "                  height_shift_range = 0.1, \n",
    "                  shear_range = 0.01,\n",
    "                  zoom_range = [0.9, 1.25],  \n",
    "                  horizontal_flip = True, \n",
    "                  vertical_flip = True,\n",
    "                  fill_mode = 'reflect',\n",
    "                   data_format = 'channels_last')\n",
    "# brightness can be problematic since it seems to change the labels differently from the images \n",
    "if AUGMENT_BRIGHTNESS:\n",
    "    dg_args[' brightness_range'] = [0.5, 1.5]\n",
    "image_gen = ImageDataGenerator(**dg_args)\n",
    "\n",
    "if AUGMENT_BRIGHTNESS:\n",
    "    dg_args.pop('brightness_range')\n",
    "label_gen = ImageDataGenerator(**dg_args)\n",
    "\n",
    "def create_aug_gen(in_gen, seed = None):\n",
    "    np.random.seed(seed if seed is not None else np.random.choice(range(9999)))\n",
    "    for in_x, in_y in in_gen:\n",
    "        seed = np.random.choice(range(9999))\n",
    "        # keep the seeds syncronized otherwise the augmentation to the images is different from the masks\n",
    "        g_x = image_gen.flow(255*in_x, \n",
    "                             batch_size = in_x.shape[0], \n",
    "                             seed = seed, \n",
    "                             shuffle=True)\n",
    "        g_y = label_gen.flow(in_y, \n",
    "                             batch_size = in_x.shape[0], \n",
    "                             seed = seed, \n",
    "                             shuffle=True)\n",
    "\n",
    "        yield next(g_x)/255.0, next(g_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cur_gen = create_aug_gen(train_gen)\n",
    "t_x, t_y = next(cur_gen)\n",
    "print('x', t_x.shape, t_x.dtype, t_x.min(), t_x.max())\n",
    "print('y', t_y.shape, t_y.dtype, t_y.min(), t_y.max())\n",
    "# only keep first 9 samples to examine in detail\n",
    "t_x = t_x[:9]\n",
    "t_y = t_y[:9]\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize = (20, 10))\n",
    "ax1.imshow(montage_rgb(t_x), cmap='gray')\n",
    "ax1.set_title('images')\n",
    "ax2.imshow(montage(t_y[:, :, :, 0]), cmap='gray_r')\n",
    "ax2.set_title('ships')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# build Model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### create upsampling layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2DTranspose\n",
    "from keras.layers import UpSampling2D\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Concatenate\n",
    "\n",
    "def handle_block_names_decode(stage):\n",
    "    conv_name = 'decoder_stage{}_conv'.format(stage)\n",
    "    bn_name = 'decoder_stage{}_bn'.format(stage)\n",
    "    relu_name = 'decoder_stage{}_relu'.format(stage)\n",
    "    up_name = 'decoder_stage{}_upsample'.format(stage)\n",
    "    return conv_name, bn_name, relu_name, up_name\n",
    "\n",
    "\n",
    "def Upsample2D_block(filters, stage, kernel_size=(3,3), upsample_rate=(2,2),\n",
    "                     batchnorm=False, skip=None):\n",
    "\n",
    "    def layer(input_tensor):\n",
    "\n",
    "        conv_name, bn_name, relu_name, up_name = handle_block_names_decode(stage)\n",
    "\n",
    "        x = UpSampling2D(size=upsample_rate, name=up_name)(input_tensor)\n",
    "\n",
    "        if skip is not None:\n",
    "            x = Concatenate()([x, skip])\n",
    "\n",
    "        x = Conv2D(filters, kernel_size, padding='same', name=conv_name+'1')(x)\n",
    "        if batchnorm:\n",
    "            x = BatchNormalization(name=bn_name+'1')(x)\n",
    "        x = Activation('relu', name=relu_name+'1')(x)\n",
    "\n",
    "        x = Conv2D(filters, kernel_size, padding='same', name=conv_name+'2')(x)\n",
    "        if batchnorm:\n",
    "            x = BatchNormalization(name=bn_name+'2')(x)\n",
    "        x = Activation('relu', name=relu_name+'2')(x)\n",
    "\n",
    "        return x\n",
    "    return layer\n",
    "\n",
    "\n",
    "def Transpose2D_block(filters, stage, kernel_size=(3,3), upsample_rate=(2,2),\n",
    "                      transpose_kernel_size=(4,4), batchnorm=False, skip=None):\n",
    "\n",
    "    def layer(input_tensor):\n",
    "\n",
    "        conv_name, bn_name, relu_name, up_name = handle_block_names_decode(stage)\n",
    "\n",
    "        x = Conv2DTranspose(filters, transpose_kernel_size, strides=upsample_rate,\n",
    "                            padding='same', name=up_name)(input_tensor)\n",
    "        if batchnorm:\n",
    "            x = BatchNormalization(name=bn_name+'1')(x)\n",
    "        x = Activation('relu', name=relu_name+'1')(x)\n",
    "\n",
    "        if skip is not None:\n",
    "            x = Concatenate()([x, skip])\n",
    "\n",
    "        x = Conv2D(filters, kernel_size, padding='same', name=conv_name+'2')(x)\n",
    "        if batchnorm:\n",
    "            x = BatchNormalization(name=bn_name+'2')(x)\n",
    "        x = Activation('relu', name=relu_name+'2')(x)\n",
    "\n",
    "        return x\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_unet(backbone, classes, last_block_filters, skip_layers,\n",
    "               n_upsample_blocks=5, upsample_rates=(2,2,2,2,2),\n",
    "               block_type='upsampling', activation='sigmoid',\n",
    "               **kwargs):\n",
    "\n",
    "    input = backbone.input\n",
    "    x = backbone.output\n",
    "    print(x)\n",
    "    if block_type == 'transpose':\n",
    "        up_block = Transpose2D_block\n",
    "    else:\n",
    "        up_block = Upsample2D_block\n",
    "\n",
    "    # convert layer names to indices\n",
    "    skip_layers = ([get_layer_number(backbone, l) if isinstance(l, str) else l\n",
    "                    for l in skip_layers])\n",
    "    for i in range(n_upsample_blocks):\n",
    "\n",
    "        # check if there is a skip connection\n",
    "        if i < len(skip_layers):\n",
    "            print(backbone.layers[skip_layers[i]])\n",
    "            print(backbone.layers[skip_layers[i]].output)\n",
    "            skip = backbone.layers[skip_layers[i]].output\n",
    "        else:\n",
    "            skip = None\n",
    "\n",
    "        up_size = (upsample_rates[i], upsample_rates[i])\n",
    "        filters = last_block_filters * 2**(n_upsample_blocks-(i+1))\n",
    "\n",
    "        x = up_block(filters, i, upsample_rate=up_size, skip=skip, **kwargs)(x)\n",
    "\n",
    "    if classes < 2:\n",
    "        activation = 'sigmoid'\n",
    "\n",
    "    x = Conv2D(classes, (3,3), padding='same', name='final_conv')(x)\n",
    "    x = Activation(activation, name=activation)(x)\n",
    "\n",
    "    model = Model(input, x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Built ResNet34 model\n",
    "#################### Reference this github"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Conv2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers import Add\n",
    "from keras.layers import ZeroPadding2D\n",
    "\n",
    "def handle_block_names(stage, block):\n",
    "    name_base = 'stage{}_unit{}_'.format(stage + 1, block + 1)\n",
    "    conv_name = name_base + 'conv'\n",
    "    bn_name = name_base + 'bn'\n",
    "    relu_name = name_base + 'relu'\n",
    "    sc_name = name_base + 'sc'\n",
    "    return conv_name, bn_name, relu_name, sc_name\n",
    "\n",
    "\n",
    "def basic_identity_block(filters, stage, block):\n",
    "\n",
    "    def layer(input_tensor):\n",
    "        conv_params = get_conv_params()\n",
    "        bn_params = get_bn_params()\n",
    "        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n",
    "        x = Activation('relu', name=relu_name + '1')(x)\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), name=conv_name + '1', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '2')(x)\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n",
    "\n",
    "        x = Add()([x, input_tensor])\n",
    "        return x\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def basic_conv_block(filters, stage, block, strides=(2, 2)):\n",
    "\n",
    "    def layer(input_tensor):\n",
    "        conv_params = get_conv_params()\n",
    "        bn_params = get_bn_params()\n",
    "        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n",
    "        x = Activation('relu', name=relu_name + '1')(x)\n",
    "        shortcut = x\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), strides=strides, name=conv_name + '1', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '2')(x)\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n",
    "\n",
    "        shortcut = Conv2D(filters, (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)\n",
    "        x = Add()([x, shortcut])\n",
    "        return x\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def conv_block(filters, stage, block, strides=(2, 2)):\n",
    "    def layer(input_tensor):\n",
    "        conv_params = get_conv_params()\n",
    "        bn_params = get_bn_params()\n",
    "        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n",
    "        x = Activation('relu', name=relu_name + '1')(x)\n",
    "        shortcut = x\n",
    "        x = Conv2D(filters, (1, 1), name=conv_name + '1', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '2')(x)\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), strides=strides, name=conv_name + '2', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '3', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '3')(x)\n",
    "        x = Conv2D(filters*4, (1, 1), name=conv_name + '3', **conv_params)(x)\n",
    "\n",
    "        shortcut = Conv2D(filters*4, (1, 1), name=sc_name, strides=strides, **conv_params)(shortcut)\n",
    "        x = Add()([x, shortcut])\n",
    "        return x\n",
    "\n",
    "    return layer\n",
    "\n",
    "\n",
    "def identity_block(filters, stage, block):\n",
    "\n",
    "    def layer(input_tensor):\n",
    "        conv_params = get_conv_params()\n",
    "        bn_params = get_bn_params()\n",
    "        conv_name, bn_name, relu_name, sc_name = handle_block_names(stage, block)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '1', **bn_params)(input_tensor)\n",
    "        x = Activation('relu', name=relu_name + '1')(x)\n",
    "        x = Conv2D(filters, (1, 1), name=conv_name + '1', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '2', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '2')(x)\n",
    "        x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "        x = Conv2D(filters, (3, 3), name=conv_name + '2', **conv_params)(x)\n",
    "\n",
    "        x = BatchNormalization(name=bn_name + '3', **bn_params)(x)\n",
    "        x = Activation('relu', name=relu_name + '3')(x)\n",
    "        x = Conv2D(filters*4, (1, 1), name=conv_name + '3', **conv_params)(x)\n",
    "\n",
    "        x = Add()([x, input_tensor])\n",
    "        return x\n",
    "\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_conv_params(**params):\n",
    "    default_conv_params = {\n",
    "        'kernel_initializer': 'glorot_uniform',\n",
    "        'use_bias': False,\n",
    "        'padding': 'valid',\n",
    "    }\n",
    "    default_conv_params.update(params)\n",
    "    return default_conv_params\n",
    "\n",
    "\n",
    "def get_bn_params(**params):\n",
    "    default_bn_params = {\n",
    "        'axis': 3,\n",
    "        'momentum': 0.99,\n",
    "        'epsilon': 2e-5,\n",
    "        'center': True,\n",
    "        'scale': True,\n",
    "    }\n",
    "    default_bn_params.update(params)\n",
    "    return default_bn_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.layers import Input\n",
    "from keras.layers import Conv2D\n",
    "from keras.layers import MaxPooling2D\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.layers import Activation\n",
    "from keras.layers import GlobalAveragePooling2D\n",
    "from keras.layers import ZeroPadding2D\n",
    "from keras.layers import Dense\n",
    "from keras.models import Model\n",
    "from keras.engine import get_source_inputs\n",
    "\n",
    "import keras\n",
    "from distutils.version import StrictVersion\n",
    "\n",
    "if StrictVersion(keras.__version__) < StrictVersion('2.2.0'):\n",
    "    from keras.applications.imagenet_utils import _obtain_input_shape\n",
    "else:\n",
    "    from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "    \n",
    "def build_resnet(\n",
    "     repetitions=(2, 2, 2, 2),\n",
    "     include_top=True,\n",
    "     input_tensor=None,\n",
    "     input_shape=None,\n",
    "     classes=1000,\n",
    "     block_type='usual'):\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=224,\n",
    "                                      min_size=197,\n",
    "                                      data_format='channels_last',\n",
    "                                      require_flatten=include_top)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = Input(shape=input_shape, name='data')\n",
    "    else:\n",
    "        if not K.is_keras_tensor(input_tensor):\n",
    "            img_input = Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "    \n",
    "    # get parameters for model layers\n",
    "    no_scale_bn_params = get_bn_params(scale=False)\n",
    "    bn_params = get_bn_params()\n",
    "    conv_params = get_conv_params()\n",
    "    init_filters = 64\n",
    "\n",
    "    if block_type == 'basic':\n",
    "        conv_block = basic_conv_block\n",
    "        identity_block = basic_identity_block\n",
    "    else:\n",
    "        conv_block = usual_conv_block\n",
    "        identity_block = usual_identity_block\n",
    "    \n",
    "    # resnet bottom\n",
    "    x = BatchNormalization(name='bn_data', **no_scale_bn_params)(img_input)\n",
    "    x = ZeroPadding2D(padding=(3, 3))(x)\n",
    "    x = Conv2D(init_filters, (7, 7), strides=(2, 2), name='conv0', **conv_params)(x)\n",
    "    x = BatchNormalization(name='bn0', **bn_params)(x)\n",
    "    x = Activation('relu', name='relu0')(x)\n",
    "    x = ZeroPadding2D(padding=(1, 1))(x)\n",
    "    x = MaxPooling2D((3, 3), strides=(2, 2), padding='valid', name='pooling0')(x)\n",
    "    \n",
    "    # resnet body\n",
    "    for stage, rep in enumerate(repetitions):\n",
    "        for block in range(rep):\n",
    "            \n",
    "            filters = init_filters * (2**stage)\n",
    "            \n",
    "            # first block of first stage without strides because we have maxpooling before\n",
    "            if block == 0 and stage == 0:\n",
    "                x = conv_block(filters, stage, block, strides=(1, 1))(x)\n",
    "                \n",
    "            elif block == 0:\n",
    "                x = conv_block(filters, stage, block, strides=(2, 2))(x)\n",
    "                \n",
    "            else:\n",
    "                x = identity_block(filters, stage, block)(x)\n",
    "                \n",
    "    x = BatchNormalization(name='bn1', **bn_params)(x)\n",
    "    x = Activation('relu', name='relu1')(x)\n",
    "\n",
    "    # resnet top\n",
    "    if include_top:\n",
    "        x = GlobalAveragePooling2D(name='pool1')(x)\n",
    "        x = Dense(classes, name='fc1')(x)\n",
    "        x = Activation('softmax', name='softmax')(x)\n",
    "\n",
    "    # Ensure that the model takes into account any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "        \n",
    "    # Create model.\n",
    "    model = Model(inputs, x)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pretrain Weight by using ImageNet trained\n",
    "Reference this GITHUB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import get_file\n",
    "\n",
    "\n",
    "def find_weights(weights_collection, model_name, dataset, include_top):\n",
    "    w = list(filter(lambda x: x['model'] == model_name, weights_collection))\n",
    "    w = list(filter(lambda x: x['dataset'] == dataset, w))\n",
    "    w = list(filter(lambda x: x['include_top'] == include_top, w))\n",
    "    return w\n",
    "\n",
    "\n",
    "def load_model_weights(weights_collection, model, dataset, classes, include_top):\n",
    "    weights = find_weights(weights_collection, model.name, dataset, include_top)\n",
    "\n",
    "    if weights:\n",
    "        weights = weights[0]\n",
    "\n",
    "        if include_top and weights['classes'] != classes:\n",
    "            raise ValueError('If using `weights` and `include_top`'\n",
    "                             ' as true, `classes` should be {}'.format(weights['classes']))\n",
    "\n",
    "        weights_path = get_file(weights['name'],\n",
    "                                weights['url'],\n",
    "                                cache_subdir='models',\n",
    "                                md5_hash=weights['md5'])\n",
    "\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    else:\n",
    "        raise ValueError('There is no weights for such configuration: ' +\n",
    "                         'model = {}, dataset = {}, '.format(model.name, dataset) +\n",
    "                         'classes = {}, include_top = {}.'.format(classes, include_top))\n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights_collection = [\n",
    "\n",
    "    # ResNet34\n",
    "    {\n",
    "        'model': 'resnet34',\n",
    "        'dataset': 'imagenet',\n",
    "        'classes': 1000,\n",
    "        'include_top': True,\n",
    "        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000.h5',\n",
    "        'name': 'resnet34_imagenet_1000.h5',\n",
    "        'md5': '2ac8277412f65e5d047f255bcbd10383',\n",
    "    },\n",
    "\n",
    "    {\n",
    "        'model': 'resnet34',\n",
    "        'dataset': 'imagenet',\n",
    "        'classes': 1000,\n",
    "        'include_top': False,\n",
    "        'url': 'https://github.com/qubvel/classification_models/releases/download/0.0.1/resnet34_imagenet_1000_no_top.h5',\n",
    "        'name': 'resnet34_imagenet_1000_no_top.h5',\n",
    "        'md5': '8caaa0ad39d927cb8ba5385bf945d582',\n",
    "    }]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Buil Unet model base on ResNet34¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Freeze Encoder weight if needed\n",
    "def freeze_model(model):\n",
    "    for layer in model.layers:\n",
    "        layer.trainable = False\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def UResNet34(input_shape=(None, None, 3), classes=1, decoder_filters=16, decoder_block_type='upsampling',\n",
    "                       encoder_weights=None, input_tensor=None, activation='sigmoid', **kwargs):\n",
    "\n",
    "    backbone = build_resnet(input_tensor=None,\n",
    "                         input_shape=input_shape,\n",
    "                         repetitions=(3, 4, 6, 3),\n",
    "                         classes=classes,\n",
    "                         include_top=False,\n",
    "                         block_type='basic')\n",
    "    backbone.name = 'resnet34'\n",
    "    \n",
    "    if encoder_weights == True:\n",
    "        load_model_weights(weights_collection, backbone , dataset= 'imagenet', classes = 1, include_top=False)\n",
    "    \n",
    "    skip_connections = list([129, 74, 37, 5]) # for resnet 34\n",
    "    model = build_unet(backbone, classes, decoder_filters,\n",
    "                       skip_connections, block_type=decoder_block_type,\n",
    "                       activation=activation, **kwargs)\n",
    "    model.name = 'u-resnet34'\n",
    "    \n",
    "    #freeze_model(backbone)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model = UResNet34(input_shape=(256,256,3),encoder_weights=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seg_model_origin.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seg_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### define Loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "\n",
    "def dice_coef(y_true, y_pred):\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    y_pred_f = K.cast(K.greater(K.flatten(y_pred), 0.5), 'float32')\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = 2. * K.sum(intersection) / (K.sum(y_true_f) + K.sum(y_pred_f))\n",
    "    return score\n",
    "\n",
    "def dice_loss(y_true, y_pred):\n",
    "    smooth = 1.\n",
    "    y_true_f = K.flatten(y_true)\n",
    "    y_pred_f = K.flatten(y_pred)\n",
    "    intersection = y_true_f * y_pred_f\n",
    "    score = (2. * K.sum(intersection) + smooth) / (K.sum(y_true_f) + K.sum(y_pred_f) + smooth)\n",
    "    return 1. - score\n",
    "\n",
    "def bce_dice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) + dice_loss(y_true, y_pred)\n",
    "\n",
    "def bce_logdice_loss(y_true, y_pred):\n",
    "    return binary_crossentropy(y_true, y_pred) - K.log(1. - dice_loss(y_true, y_pred))\n",
    "\n",
    "def weighted_bce_loss(y_true, y_pred, weight):\n",
    "    epsilon = 1e-7\n",
    "    y_pred = K.clip(y_pred, epsilon, 1. - epsilon)\n",
    "    logit_y_pred = K.log(y_pred / (1. - y_pred))\n",
    "    loss = weight * (logit_y_pred * (1. - y_true) + \n",
    "                     K.log(1. + K.exp(-K.abs(logit_y_pred))) + K.maximum(-logit_y_pred, 0.))\n",
    "    return K.sum(loss) / K.sum(weight)\n",
    "\n",
    "def weighted_dice_loss(y_true, y_pred, weight):\n",
    "    smooth = 1.\n",
    "    w, m1, m2 = weight, y_true, y_pred\n",
    "    intersection = (m1 * m2)\n",
    "    score = (2. * K.sum(w * intersection) + smooth) / (K.sum(w * m1) + K.sum(w * m2) + smooth)\n",
    "    loss = 1. - K.sum(score)\n",
    "    return loss\n",
    "\n",
    "def weighted_bce_dice_loss(y_true, y_pred):\n",
    "    y_true = K.cast(y_true, 'float32')\n",
    "    y_pred = K.cast(y_pred, 'float32')\n",
    "    # if we want to get same size of output, kernel size must be odd\n",
    "    averaged_mask = K.pool2d(\n",
    "            y_true, pool_size=(50, 50), strides=(1, 1), padding='same', pool_mode='avg')\n",
    "    weight = K.ones_like(averaged_mask)\n",
    "    w0 = K.sum(weight)\n",
    "    weight = 5. * K.exp(-5. * K.abs(averaged_mask - 0.5))\n",
    "    w1 = K.sum(weight)\n",
    "    weight *= (w0 / w1)\n",
    "    loss = weighted_bce_loss(y_true, y_pred, weight) + dice_loss(y_true, y_pred)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "def dice_coef(y_true, y_pred, smooth=1):\n",
    "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3])\n",
    "    return K.mean( (2. * intersection + smooth) / (union + smooth), axis=0)\n",
    "def dice_p_bce(in_gt, in_pred):\n",
    "    return 1e-3*binary_crossentropy(in_gt, in_pred) - dice_coef(in_gt, in_pred)\n",
    "def true_positive_rate(y_true, y_pred):\n",
    "    return K.sum(K.flatten(y_true)*K.flatten(K.round(y_pred)))/K.sum(y_true)\n",
    "#seg_model.compile(optimizer=Adam(1e-4, decay=1e-6), loss=dice_p_bce, metrics=[dice_coef, 'binary_accuracy', true_positive_rate])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# set training check point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path = path + 'models/seg_model_Unet34.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.callbacks import ModelCheckpoint, LearningRateScheduler, EarlyStopping, ReduceLROnPlateau\n",
    "#weight_path=path + \"/models/{}_weights.best.hdf5\".format('seg_model_Unet34')\n",
    "\n",
    "checkpoint = ModelCheckpoint(weight_path, monitor='val_dice_coef', verbose=1, \n",
    "                             save_best_only=True, mode='max', save_weights_only = True)\n",
    "\n",
    "reduceLROnPlat = ReduceLROnPlateau(monitor='val_dice_coef', factor=0.5, \n",
    "                                   patience=3, \n",
    "                                   verbose=1, mode='max', epsilon=0.0001, cooldown=2, min_lr=1e-6)\n",
    "early = EarlyStopping(monitor=\"val_dice_coef\", \n",
    "                      mode=\"max\", \n",
    "                      patience=15) # probably needs to be more patient, but kaggle time is limited\n",
    "callbacks_list = [checkpoint, early, reduceLROnPlat]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comiple model\n",
    "seg_model.compile(optimizer=Adam(1e-4, decay=1e-6), loss=bce_logdice_loss, metrics=[dice_coef, 'binary_accuracy', true_positive_rate])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unet34 train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras.backend as K\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import binary_crossentropy\n",
    "\n",
    "## intersection over union\n",
    "def IoU(y_true, y_pred, eps=1e-6):\n",
    "    #print(y_true)\n",
    "    #if np.max(y_true) == 0.0:\n",
    "    #    return IoU(1-y_true, 1-y_pred) ## empty image; calc IoU of zeros\n",
    "    intersection = K.sum(y_true * y_pred, axis=[1,2,3])\n",
    "    union = K.sum(y_true, axis=[1,2,3]) + K.sum(y_pred, axis=[1,2,3]) - intersection\n",
    "    return -K.mean( (intersection + eps) / (union + eps), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "seg_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "batch_size = 32\n",
    "\n",
    "step_count = min(MAX_TRAIN_STEPS, balanced_train_df.shape[0]//BATCH_SIZE)\n",
    "aug_gen = create_aug_gen(make_image_gen(train_df))\n",
    "loss_history = [seg_model.fit_generator(aug_gen,\n",
    "                            steps_per_epoch=step_count,\n",
    "                            validation_data=(valid_x, valid_y), \n",
    "                            epochs=5,\n",
    "                            callbacks=callbacks_list,shuffle=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "'''\n",
    "def fit():\n",
    "    seg_model.compile(optimizer=Adam(1e-3, decay=1e-6), loss=IoU, metrics=['binary_accuracy'])\n",
    "    \n",
    "    step_count = min(MAX_TRAIN_STEPS, train_df.shape[0]//BATCH_SIZE)\n",
    "    aug_gen = create_aug_gen(make_image_gen(train_df))\n",
    "    loss_history = [seg_model.fit_generator(aug_gen,\n",
    "                                 steps_per_epoch=step_count,\n",
    "                                 epochs=MAX_TRAIN_EPOCHS,\n",
    "                                 validation_data=(valid_x, valid_y),\n",
    "                                 callbacks=callbacks_list,\n",
    "                                workers=1 # the generator is not very thread safe\n",
    "                                           )]\n",
    "    return loss_history\n",
    "i = 0\n",
    "\n",
    "while True:\n",
    "    if i==1: break\n",
    "    i+=1\n",
    "    loss_history = fit()\n",
    "    if np.min([mh.history['val_loss'] for mh in loss_history]) < -0.2:\n",
    "        break\n",
    "        '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weight_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model.load_weights(weight_path)\n",
    "seg_model.save(weight_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_loss(loss_history):\n",
    "    epochs = np.concatenate([mh.epoch for mh in loss_history])\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(22, 10))\n",
    "    \n",
    "    _ = ax1.plot(epochs, np.concatenate([mh.history['loss'] for mh in loss_history]), 'b-',\n",
    "                 epochs, np.concatenate([mh.history['val_loss'] for mh in loss_history]), 'r-')\n",
    "    ax1.legend(['Training', 'Validation'])\n",
    "    ax1.set_title('Loss')\n",
    "    \n",
    "    _ = ax2.plot(epochs, np.concatenate([mh.history['binary_accuracy'] for mh in loss_history]), 'b-',\n",
    "                 epochs, np.concatenate([mh.history['val_binary_accuracy'] for mh in loss_history]), 'r-')\n",
    "    ax2.legend(['Training', 'Validation'])\n",
    "    ax2.set_title('Binary Accuracy (%)')\n",
    "\n",
    "show_loss(loss_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seg_model.load_weights(weight_path)\n",
    "seg_model.save(path + '/models/seg_model_Unet34.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_y = seg_model.predict(valid_x)\n",
    "print(pred_y.shape, pred_y.min(axis=0).max(), pred_y.max(axis=0).min(), pred_y.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize = (6, 6))\n",
    "ax.hist(pred_y.ravel(), np.linspace(0, 1, 20))\n",
    "ax.set_xlim(0, 1)\n",
    "ax.set_yscale('log', nonposy='clip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prepare Full Resolution Model\n",
    "Here we account for the scaling so everything can happen in the model itself"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "if IMG_SCALING is not None:\n",
    "    fullres_model_origin = models.Sequential()\n",
    "    fullres_model_origin.add(layers.AvgPool2D(IMG_SCALING, input_shape = (None, None, 3)))\n",
    "    fullres_model_origin.add(seg_model_origin)\n",
    "    fullres_model_origin.add(layers.UpSampling2D(IMG_SCALING))\n",
    "else:\n",
    "    fullres_model_origin = seg_model_origin\n",
    "fullres_model_origin.save(path + '/models/seg_model_Unet34.h5')\n",
    "fullres_model_origin.summary()\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IMG_SCALING is not None:\n",
    "    fullres_model = models.Sequential()\n",
    "    fullres_model.add(layers.AvgPool2D(IMG_SCALING, input_shape = (None, None, 3)))\n",
    "    fullres_model.add(seg_model)\n",
    "    fullres_model.add(layers.UpSampling2D(IMG_SCALING))\n",
    "else:\n",
    "    fullres_model = seg_model\n",
    "fullres_model.save(path + '/models/seg_model_Unet34_fullness.h5')\n",
    "fullres_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize predictions¶"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def raw_prediction(img, path=test_image_dir):\n",
    "    #print('raw_prediction')\n",
    "    c_img = imread(os.path.join(path, c_img_name))\n",
    "    #print('c_img->?',c_img.shape)\n",
    "    c_img = np.expand_dims(c_img, 0)/255.0\n",
    "    #print('c_img->?',c_img.shape)\n",
    "    #print('c_img->',c_img.shape,c_img[0].shape)\n",
    "    cur_seg = fullres_model.predict(c_img)[0]\n",
    "    #print('return')\n",
    "    return cur_seg, c_img[0]\n",
    "\n",
    "def smooth(cur_seg):\n",
    "    return binary_opening(cur_seg>0.99, np.expand_dims(disk(2), -1))\n",
    "\n",
    "def predict(img, path=test_image_dir):\n",
    "    cur_seg, c_img = raw_prediction(img, path=path)\n",
    "    return smooth(cur_seg), c_img\n",
    "\n",
    "## Get a sample of each group of ship count\n",
    "samples = valid_df.groupby('ships').apply(lambda x: x.sample(1))\n",
    "fig, m_axs = plt.subplots(samples.shape[0], 4, figsize = (15, samples.shape[0]*4))\n",
    "[c_ax.axis('off') for c_ax in m_axs.flatten()]\n",
    "\n",
    "for (ax1, ax2, ax3, ax4), c_img_name in zip(m_axs, samples.ImageId.values):\n",
    "    #print('c_img_name->', ax1)\n",
    "    first_seg, first_img = raw_prediction(c_img_name, train_image_dir)\n",
    "    ax1.imshow(first_img)\n",
    "    ax1.set_title('Image: ' + c_img_name)\n",
    "    ax2.imshow(first_seg[:, :, 0], cmap=get_cmap('jet'))\n",
    "    ax2.set_title('Model Prediction')\n",
    "    reencoded = masks_as_color(multi_rle_encode(smooth(first_seg)[:, :, 0]))\n",
    "    ax3.imshow(reencoded)\n",
    "    ax3.set_title('Prediction Masks')\n",
    "    ground_truth = masks_as_color(masks.query('ImageId==\"{}\"'.format(c_img_name))['EncodedPixels'])\n",
    "    ax4.imshow(ground_truth)\n",
    "    ax4.set_title('Ground Truth')\n",
    "    \n",
    "fig.savefig(path + '/pic/validation.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "have_ship= pd.read_csv(\"Have_ship_or_not.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_names = have_ship.loc[have_ship['Have_ship'] > 0.5, ['ImageId']]['ImageId'].values.tolist()\n",
    "test_names_nothing = have_ship.loc[have_ship['Have_ship'] <= 0.5, ['ImageId']]['ImageId'].values.tolist()\n",
    "len(test_names), len(test_names_nothing)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ship_list_dict = []\n",
    "for name in test_names_nothing:\n",
    "    ship_list_dict.append({'ImageId':name,'EncodedPixels':None})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths = np.array(os.listdir(test_image_dir))\n",
    "print(len(test_paths), 'test images found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "from tqdm import tqdm_notebook\n",
    "from skimage.morphology import binary_opening, disk\n",
    "for c_img_name in tqdm_notebook(test_paths):\n",
    "    if c_img_name in test_names:\n",
    "        c_path = os.path.join(test_image_dir, c_img_name)\n",
    "        c_img = imread(c_path)\n",
    "        c_img = np.expand_dims(c_img, 0)/255.0\n",
    "        cur_seg = fullres_model.predict(c_img)[0]\n",
    "        cur_seg = binary_opening(cur_seg>0.5, np.expand_dims(disk(2), -1))\n",
    "        cur_rles = multi_rle_encode(cur_seg)\n",
    "        if len(cur_rles)>0:\n",
    "            for c_rle in cur_rles:\n",
    "                ship_list_dict += [{'ImageId': c_img_name, 'EncodedPixels': c_rle}]\n",
    "        else:\n",
    "            ship_list_dict += [{'ImageId': c_img_name, 'EncodedPixels': 'None'}]\n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(ship_list_dict)[['ImageId', 'EncodedPixels']]\n",
    "submission_df.to_csv('submission.csv', index=False)\n",
    "submission_df.sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.loc[submission_df['EncodedPixels'] != None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_paths = np.array(os.listdir(test_image_dir))\n",
    "print(len(test_paths), 'test images found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook\n",
    "\n",
    "def pred_encode(img, **kwargs):\n",
    "    cur_seg, _ = predict(img)\n",
    "    cur_rles = multi_rle_encode(cur_seg, **kwargs)\n",
    "    return [[img, rle] for rle in cur_rles if rle is not None]\n",
    "\n",
    "out_pred_rows = []\n",
    "for c_img_name in tqdm_notebook(test_paths[:30000]): ## only a subset as it takes too long to run\n",
    "    out_pred_rows += pred_encode(c_img_name, min_max_threshold=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub = pd.DataFrame(out_pred_rows)\n",
    "sub.columns = ['ImageId', 'EncodedPixels']\n",
    "sub = sub[sub.EncodedPixels.notnull()]\n",
    "sub.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## let's see what we got\n",
    "TOP_PREDICTIONS=5\n",
    "fig, m_axs = plt.subplots(TOP_PREDICTIONS, 2, figsize = (9, TOP_PREDICTIONS*5))\n",
    "[c_ax.axis('off') for c_ax in m_axs.flatten()]\n",
    "\n",
    "for (ax1, ax2), c_img_name in zip(m_axs, sub.ImageId.unique()[:TOP_PREDICTIONS]):\n",
    "    c_img = imread(os.path.join(test_image_dir, c_img_name))\n",
    "    c_img = np.expand_dims(c_img, 0)/255.0\n",
    "    ax1.imshow(c_img[0])\n",
    "    ax1.set_title('Image: ' + c_img_name)\n",
    "    ax2.imshow(masks_as_color(sub.query('ImageId==\"{}\"'.format(c_img_name))['EncodedPixels']))\n",
    "    ax2.set_title('Prediction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub1 = pd.read_csv(path + '/sample_submission_v2.csv')\n",
    "sub1 = pd.DataFrame(np.setdiff1d(sub1['ImageId'].unique(), sub['ImageId'].unique(), assume_unique=True), columns=['ImageId'])\n",
    "sub1['EncodedPixels'] = None\n",
    "print(len(sub1), len(sub))\n",
    "\n",
    "sub = pd.concat([sub, sub1])\n",
    "print(len(sub))\n",
    "sub.to_csv('submission_02.csv', index=False)\n",
    "sub.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
